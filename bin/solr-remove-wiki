#!/usr/bin/perl -w

=pod

This script will remove all existing documents in the solr index for a given wiki or wikis.  This is useful for making sure the content of already deleted wikis is no longer taking up space in the index.  It could also be used to remove content from wikis considered dead or inactive but not ready to be deleted.

=cut

use strict;

use Encode;
use LWP::UserAgent;
use Getopt::Long;
use HTTP::Request;
use JSON::XS;

use constant API_MAX_ROWS    => 500;
use constant SOLR_MAX_ROWS   => 10_000;
use constant SOLR_HOST       => '10.8.42.26:8983';
use constant SOLR_SELECT_URL => 'http://'.SOLR_HOST.'/solr/select';
use constant SOLR_UPDATE_URL => 'http://'.SOLR_HOST.'/solr/update';

our $VERBOSE = 0;
our $SOLR_UA;
our @FAILURES;

GetOptions('verbose|v' => \$VERBOSE,
           'help|h'    => sub { help(); exit }
          );

my @wikis = @ARGV;

# Test to see if we're being called via a pipe
if (not (-t STDIN  && -t STDOUT)) {
	if (!@wikis) {
		@wikis = <STDIN>;
		chomp @wikis;
	}
}

if (!@wikis) {
	die "A list of wikis to remove is needed on the command line or via STDIN\n";
}

init_user_agents();

my $total = scalar @wikis;
my $width = length($total);
my $num = 1;
my $pages = 0;

foreach my $w (@wikis) {
	printf("== (%${width}d/%d) Removing %s\n", $num++, $total, $w) if $VERBOSE;
	my ($host, $wid) = split(':', $w);

	# Get the host (the wiki db name plus the domain
	$host .= ($host =~ m!\.(com|org)$! ? '' : '.wikia.com');

	# Remove any trailing slash
	$host =~ s!/$!!;
	# Remove any protocol
	$host =~ s!^http://!!;

	my $page_ids = fetch_from_solr($host, $wid);
	next unless $page_ids;

	printf("-- Removing %d pages from the index\n", scalar @$page_ids);

	$pages += scalar @$page_ids;

	my $xml = '<delete>';
	foreach my $id (@$page_ids) {
		$xml .= "<query>wid:$wid AND pageid:$id</query>"
	}
	$xml .= '</delete>';

	post($xml);
}

print "Removed $pages total pages across $total wikis\n" if $VERBOSE;

if (@FAILURES) {
	print "Failed while attempting to remove the following wikis:\n";
	print join("\n", @FAILURES)."\n";
}


################################################################################

sub help {

}

sub init_user_agents {
	$SOLR_UA = LWP::UserAgent->new;
	$SOLR_UA->parse_head(0);
	$SOLR_UA->timeout(10);
	$SOLR_UA->requests_redirectable('');
	$SOLR_UA->env_proxy();
}

sub fetch_from_solr {
	my ($host, $wid) = @_;
	my %page_ids;
	my $offset = 0;

	print "-- Fetching indexed pages " if $VERBOSE;

	while (1) {
		print "." if $VERBOSE;
		my $res = request(SOLR_SELECT_URL(),
						  q     => 'host:'.$host,
						  fq    => 'wid:'.$wid,
						  fl    => 'pageid',
						  wt    => 'json',
						  rows  => SOLR_MAX_ROWS(),
						  start => $offset
						 );
		unless ($res) {
			push @FAILURES, $host.':'.$wid;
			return;
		}

		my ($j, $docs);       
		$j = decode_json($res->content) if $res->content;
		$docs = $j->{'response'}->{'docs'} if $j;

		if ($docs) {
			$offset += SOLR_MAX_ROWS();
			foreach my $d (@$docs) {
				$page_ids{$d->{pageid}} = 1;
			}
			
			last if @$docs != SOLR_MAX_ROWS() 
		} else {
			last;
		}
	}

	print " done\n" if $VERBOSE;

	return %page_ids ? [keys %page_ids] : undef;
}

sub find_redirects {
	my ($host, $page_id) = @_;
	my $title = get_index_title($host, $page_id);

	return unless $title;

	# Escape title characters for SOLR
	$title =~ s/\Q+-&|!(){}[]^"~*?:\\\E/\\$1/g;

	my $req = HTTP::Request->new(GET => SOLR_SELECT_URL);
	$req->url->query_form('q'  => "canonical:${title} AND host:${host}",
						  'fq' => "canonical:${title}+AND+host:${host}",
			  			  'fl' => 'pageid',
			  			  'wt' => 'json');
	my $res = $SOLR_UA->request($req);
	return unless $res->is_success;

	my $j = decode_json($res->content);
	return unless $j->{response}->{numFound} > 0;

	my $docs = $j->{response}->{docs};
	my %ids;
	foreach my $doc (@{$docs}) {
		$ids{$doc->{pageid}} = 1;
	}

	return %ids ? [keys %ids] : undef;
}

sub get_index_title {
	my ($host, $page_id) = @_;

	my $req = HTTP::Request->new(GET => SOLR_SELECT_URL);
	$req->url->query_form('q'  => "host:$host AND pageid:$page_id",
						  'fl' => 'title',
						  'wt' => 'json');
	my $res = $SOLR_UA->request($req);
	return 0 unless $res->is_success;

	my $j = decode_json($res->content);
	return 0 unless $j->{response}->{numFound} > 0;
	
	my $docs = $j->{response}->{docs};
	return $docs->[0]->{title};
}

sub request {
	my ($url, %params) = @_;

	my $req = HTTP::Request->new(GET => $url);
	$req->url->query_form(%params);

	my $res = $SOLR_UA->request($req);

	if (! $res->is_success) {
		warn "Bad response from $url: ".$res->status_line."\n";
		return;
	}

	return $res;
}

sub post {
	my ($xml) = @_;

	my $req = HTTP::Request->new(POST => SOLR_UPDATE_URL);
	$req->header("Content-Type" => "application/xml; charset=utf-8");
	$req->content(encode("UTF-8", $xml));
	my $res = $SOLR_UA->request($req);

	return 1 if $res->is_success;
	
	print STDERR "($$) Post failed: ".$res->status_line."\n";
	return 0;
}

